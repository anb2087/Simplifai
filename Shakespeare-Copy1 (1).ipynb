{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,cv2,re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import pickle\n",
    "from fastai.text import*\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says we will use GPU number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START OF MEDICAL STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opens and reads in the file with all of the medical reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"_id\" : { \"$oid\" : \"55ff25239ad446c56ba8b321\" }, \"hl7_json_history\" : [], \"finalized_date\" : null,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fh = open('/home/hoc2003/trove-data/trove-studies.json')\n",
    "studies_string = fh.read()\n",
    "fh.close()\n",
    "studies_string[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each medical report we take only the section called finalized_report and put it into a new list while keeping track of the number of files that do not have a finalized report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_array = []\n",
    "temp = None\n",
    "counter=0\n",
    "for study in studies_string.split('\\n'):\n",
    "    try:\n",
    "        temp = json.loads(study)\n",
    "        t=temp['finalized_report']\n",
    "        if t != None:\n",
    "            report_array.append(t)\n",
    "        #report_array.append(temp['finalized_report'])\n",
    "    except:\n",
    "        counter=counter+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in PA and LAT reports and AP reports that we then put in seperate lists based on the ID codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_array = []\n",
    "front_array = []\n",
    "counter = 0\n",
    "for x in report_array:\n",
    "    if re.search(r'DXCHPALAT', x) != None:\n",
    "        lat_array.append(x)\n",
    "    if re.search(r'DXMCH1V', x) != None:\n",
    "        front_array.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that returns a list of all the impressions, a list of reports that didn't have impressions, and the number of reports that didn't have impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impressions(arr):\n",
    "    lst=[]\n",
    "    other=[]\n",
    "    counter=0\n",
    "    for x in arr:\n",
    "        if re.search(r'IMPRESSION\\:(.*?)Prepared By',x)!= None:\n",
    "            y=re.search(r'IMPRESSION\\:(.*?)Prepared By',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'Impression\\:(.*?)Prepared By',x) !=None:\n",
    "            y=re.search(r'Impression\\:(.*?)Prepared By',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'Impression(.*?)Prepared By',x) != None:\n",
    "            y=re.search(r'Impression(.*?)Prepared By',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'IMPRESSION(.*?)Prepared By',x) != None:\n",
    "            y=re.search(r'IMPRESSION(.*?)Prepared By',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        else:\n",
    "            counter=counter+1\n",
    "            other.append(x)\n",
    "    return lst,other,counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that returns a lost of all the findings, a list of reports that didn't have findings, and the number of reports that didn't have findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_findings(arr):\n",
    "    lst=[]\n",
    "    other=[]\n",
    "    counter=0\n",
    "    for x in arr:\n",
    "        if re.search(r'Findings\\:(.*?)Impression',x)!= None:\n",
    "            y=re.search(r'Findings\\:(.*?)Impression',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'Findings \\:(.*?)Impression',x)!= None:\n",
    "            y=re.search(r'Findings \\:(.*?)Impression',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'FINDINGS\\:(.*?)IMPRESSION',x) != None:\n",
    "            y=re.search(r'FINDINGS\\:(.*?)IMPRESSION',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'Findings\\:(.*?)IMPRESSION',x) != None:\n",
    "            y=re.search(r'Findings\\:(.*?)IMPRESSION',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'FINDINGS/IMPRESSION\\:(.*?)Prepared By',x) != None:\n",
    "            y=re.search(r'FINDINGS/IMPRESSION\\:(.*?)Prepared By',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'FINDINGS / IMPRESSION\\:(.*?)Prepared By',x) != None:\n",
    "            y=re.search(r'FINDINGS / IMPRESSION\\:(.*?)Prepared By',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'Findings/impression\\:(.*?)Prepared By',x) != None:\n",
    "            y=re.search(r'Findings/impression\\:(.*?)Prepared By',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'Findings/Impression\\:(.*?)Prepared By',x) != None:\n",
    "            y=re.search(r'Findings/Impression\\:(.*?)Prepared By',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        elif re.search(r'Findings and Impression\\:(.*?)Prepared By',x) != None:\n",
    "            y=re.search(r'Findings and Impression\\:(.*?)Prepared By',x).group(1)\n",
    "            z=y.replace('|',' ').strip()\n",
    "            lst.append(re.sub(' +',' ',z))\n",
    "        else:\n",
    "            counter=counter+1\n",
    "            other.append(x)\n",
    "    return lst,other,counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 4 cells respectivly represent the PA and LAT impression, the PA impression, the PA and LAT findings, and then the AP findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_im, lat_o, lic = get_impressions(lat_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_im, front_o, fic = get_impressions(front_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_fin, lat_fo, lfc = get_findings(lat_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_fin, front_fo, ffc = get_findings(front_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that returns the origional string where the first character is lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_lower(s):\n",
    "   if len(s) == 0:\n",
    "      return s\n",
    "   else:\n",
    "      return s[0].lower() + s[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Function that returns the origional string where the first character is uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_upper(s):\n",
    "   if len(s) == 0:\n",
    "      return s\n",
    "   else:\n",
    "      return s[0].upper() + s[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads in the dictionary and removes any quotation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open('rad-dict.csv', 'r'))\n",
    "d = {}\n",
    "for row in reader:\n",
    "   k, v = row\n",
    "   d[k.replace('\"','')]=v.replace('\"','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that takes in a string and a dictionary and for each key in the dictionary it checks to see if it is in the string and if it is it will replace that key with its value in the corresponding format (uppercase vs lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        if re.search(r'\\b' + i + r'\\b', text):\n",
    "            text = text.replace(i, first_lower(j))\n",
    "        elif re.search(r'\\b' + first_upper(i) + r'\\b', text):\n",
    "            text = text.replace(first_upper(i), first_upper(j))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that takes in a list and creates a new list after applying the replace_all method to the original list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_report(arr):\n",
    "    mod=[]\n",
    "    for re in arr:\n",
    "        mod.append(replace_all(re, d))\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying mod_report to the PA and LAT impression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lat_im = mod_report(lat_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipping and unzipping lists, qs contains the tuples of original and modified reports also start of Fast.ai influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped=zip(lat_im,new_lat_im)\n",
    "qs=list(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_qs, n_qs = zip(*qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_tok = Tokenizer.proc_all((o_qs),'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tok = Tokenizer.proc_all((n_qs),'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['tubes',\n",
       "  'and',\n",
       "  'lines',\n",
       "  ':',\n",
       "  'none',\n",
       "  '.',\n",
       "  'lungs',\n",
       "  '/',\n",
       "  'pleura',\n",
       "  ':',\n",
       "  'lung',\n",
       "  'volumes',\n",
       "  'are',\n",
       "  'slightly',\n",
       "  'low',\n",
       "  ',',\n",
       "  'which',\n",
       "  'results',\n",
       "  'in',\n",
       "  'mild',\n",
       "  'crowding',\n",
       "  'of',\n",
       "  'the',\n",
       "  'perihilar',\n",
       "  'vasculature',\n",
       "  '.',\n",
       "  'there',\n",
       "  'is',\n",
       "  'no',\n",
       "  'new',\n",
       "  'focal',\n",
       "  'airspace',\n",
       "  'consolidation',\n",
       "  '.',\n",
       "  'no',\n",
       "  'pleural',\n",
       "  'effusion',\n",
       "  'or',\n",
       "  'pneumothorax',\n",
       "  'is',\n",
       "  'identified',\n",
       "  '.',\n",
       "  'a',\n",
       "  'calcified',\n",
       "  'nodule',\n",
       "  'in',\n",
       "  'the',\n",
       "  'periphery',\n",
       "  'of',\n",
       "  'the',\n",
       "  'right',\n",
       "  'lung',\n",
       "  'is',\n",
       "  'unchanged',\n",
       "  '.',\n",
       "  'mediastinum',\n",
       "  ':',\n",
       "  'the',\n",
       "  'cardiomediastinal',\n",
       "  'silhouette',\n",
       "  'is',\n",
       "  'at',\n",
       "  'the',\n",
       "  'upper',\n",
       "  'limits',\n",
       "  'of',\n",
       "  'normal',\n",
       "  'for',\n",
       "  'size',\n",
       "  '.',\n",
       "  'other',\n",
       "  ':',\n",
       "  'no',\n",
       "  'displaced',\n",
       "  'rib',\n",
       "  'fracture',\n",
       "  'is',\n",
       "  'identified',\n",
       "  ',',\n",
       "  'however',\n",
       "  'in',\n",
       "  'the',\n",
       "  'setting',\n",
       "  'of',\n",
       "  'chest',\n",
       "  'pain',\n",
       "  'if',\n",
       "  'there',\n",
       "  'is',\n",
       "  'high',\n",
       "  'clinical',\n",
       "  'suspicion',\n",
       "  'for',\n",
       "  'rib',\n",
       "  'injury',\n",
       "  ',',\n",
       "  'a',\n",
       "  'dedicated',\n",
       "  'rib',\n",
       "  'series',\n",
       "  'is',\n",
       "  'recommended',\n",
       "  '.'],\n",
       " ['tubes',\n",
       "  'and',\n",
       "  'lines',\n",
       "  ':',\n",
       "  'none',\n",
       "  '.',\n",
       "  'lungs',\n",
       "  '/',\n",
       "  'pleura',\n",
       "  ':',\n",
       "  'lung',\n",
       "  'volumes',\n",
       "  'are',\n",
       "  'slightly',\n",
       "  'low',\n",
       "  ',',\n",
       "  'which',\n",
       "  'results',\n",
       "  'in',\n",
       "  'mild',\n",
       "  'crowding',\n",
       "  'of',\n",
       "  'the',\n",
       "  'perihilar',\n",
       "  'vessels',\n",
       "  '.',\n",
       "  'there',\n",
       "  'is',\n",
       "  'no',\n",
       "  'new',\n",
       "  'focal',\n",
       "  'airspace',\n",
       "  'consolidation',\n",
       "  '.',\n",
       "  'no',\n",
       "  'fluid',\n",
       "  'around',\n",
       "  'the',\n",
       "  'lungs',\n",
       "  'or',\n",
       "  'partial',\n",
       "  'collapse',\n",
       "  'of',\n",
       "  'lung',\n",
       "  'is',\n",
       "  'identified',\n",
       "  '.',\n",
       "  'a',\n",
       "  'calcified',\n",
       "  'nodule',\n",
       "  'in',\n",
       "  'the',\n",
       "  'periphery',\n",
       "  'of',\n",
       "  'the',\n",
       "  'right',\n",
       "  'lung',\n",
       "  'is',\n",
       "  'unchanged',\n",
       "  '.',\n",
       "  'mediastinum',\n",
       "  ':',\n",
       "  'the',\n",
       "  'heart',\n",
       "  'shadow',\n",
       "  'is',\n",
       "  'at',\n",
       "  'the',\n",
       "  'upper',\n",
       "  'limits',\n",
       "  'of',\n",
       "  'normal',\n",
       "  'for',\n",
       "  'size',\n",
       "  '.',\n",
       "  'other',\n",
       "  ':',\n",
       "  'no',\n",
       "  'displaced',\n",
       "  'rib',\n",
       "  'fracture',\n",
       "  'is',\n",
       "  'identified',\n",
       "  ',',\n",
       "  'however',\n",
       "  'in',\n",
       "  'the',\n",
       "  'setting',\n",
       "  'of',\n",
       "  'chest',\n",
       "  'pain',\n",
       "  'if',\n",
       "  'there',\n",
       "  'is',\n",
       "  'high',\n",
       "  'clinical',\n",
       "  'suspicion',\n",
       "  'for',\n",
       "  'rib',\n",
       "  'injury',\n",
       "  ',',\n",
       "  'a',\n",
       "  'dedicated',\n",
       "  'rib',\n",
       "  'series',\n",
       "  'is',\n",
       "  'recommended',\n",
       "  '.'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ol_tok[4], ne_tok[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS TO GET COMMON WORDS FOR DICTIONARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ol_tok is a list of a list of tokens for where each list is a different report so we are creating a new list of tokens from ol_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_t=[]\n",
    "for o in ol_tok:\n",
    "    for x in o:\n",
    "        ol_t.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making the list of tokens into a collections object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "ol_count=collections.Counter(ol_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the collections method most common and putting the most commin 2000 words in a pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co=ol_count.most_common(2000)\n",
    "pickle.dump(co, open('lat_counts.csv', 'wb'))\n",
    "print(co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the same steps but of the AP impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fron_tok = Tokenizer.proc_all((front_im),'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fron_t=[]\n",
    "for o in fron_tok:\n",
    "    for x in o:\n",
    "        fron_t.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "fron_count=collections.Counter(fron_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fron_count.most_common(2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACK TO FAST-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.0, 36.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile([len(o) for o in ol_tok], 90), np.percentile([len(o) for o in ne_tok], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = np.array([len(o) for o in ol_tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_tok = np.array(ol_tok)[keep]\n",
    "ne_tok = np.array(ne_tok)[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling ol_tok and ne_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name1= \"1origional.pkl\"\n",
    "fileObject1=open(file_name1, 'wb')\n",
    "pickle.dump(ol_tok,fileObject1)\n",
    "file_name2= \"1modified.pkl\"\n",
    "fileObject2=open(file_name2, 'wb')\n",
    "pickle.dump(ne_tok,fileObject2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ol_tok, (Path('1origional').open('wb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ne_tok, (Path('1modified').open('wb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_tok = pickle.load((Path('1origional')).open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tok = pickle.load((Path('1modified')).open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject1.close()\n",
    "fileObject1=open(file_name1, 'rb')\n",
    "ol_tok=pickle.load(fileObject1)\n",
    "fileObject2.close()\n",
    "fileObject2=open(file_name2, 'rb')\n",
    "ne_tok=pickle.load(fileObject2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting each token into a number and adding some pading and beginning and end of string ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1=Path('fastai/courses/andrea/data/translation/tmp')\n",
    "def toks2ids(tok,pre):\n",
    "    freq = Counter(p for o in tok for p in o)\n",
    "    itos = [o for o,c in freq.most_common(40000)]\n",
    "    itos.insert(0, '_bos_')\n",
    "    itos.insert(1, '_pad_')\n",
    "    itos.insert(2, '_eos_')\n",
    "    itos.insert(3, '_unk')\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "    temp = None\n",
    "    ids = []\n",
    "    for p in tok:\n",
    "        if p != None:\n",
    "            temp = []\n",
    "            for o in p:\n",
    "                temp.append(stoi[o])\n",
    "            temp += [2]\n",
    "            ids.append(temp)\n",
    "    ids = np.array(ids)\n",
    "    print(len(ids))\n",
    "    np.save(f'{pre}_ids.npy', ids)\n",
    "    pickle.dump(itos, open(f'{pre}_itos.pkl', 'wb'))\n",
    "    return ids,itos,stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already done this process you nead to remove the files before doing it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'en4_ids.npy': No such file or directory\r\n",
      "rm: cannot remove 'en4_itos.pkl': No such file or directory\r\n",
      "rm: cannot remove 'en5_ids.npy': No such file or directory\r\n",
      "rm: cannot remove 'en5_itos.pkl': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "rm en4_ids.npy en4_itos.pkl en5_ids.npy en5_itos.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100662\n",
      "100662\n"
     ]
    }
   ],
   "source": [
    "ol_ids,ol_itos,ol_stoi = toks2ids(ol_tok,'en4')\n",
    "ne_ids,ne_itos,ne_stoi = toks2ids(ne_tok,'en5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ids(pre):\n",
    "    ids = np.load(f'{pre}_ids.npy')\n",
    "    itos = pickle.load(open(f'{pre}_itos.pkl', 'rb'))\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "    return ids,itos,stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_ids,ol_itos,ol_stoi = load_ids('en4')\n",
    "ne_ids,ne_itos,ne_stoi = load_ids('en5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['left', 'ventricular', 'shunt', 'catheter', 'as', 'described', '.', '_eos_'],\n",
       " ['left', 'ventricular', 'shunt', 'catheter', 'as', 'described', '.', '_eos_'],\n",
       " 441,\n",
       " 432)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ol_itos[o] for o in ol_ids[0]], [ne_itos[p] for p in ne_ids[0]], len(ol_itos), len(ne_itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import word vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs = ft.load_model(str('data/translate/wiki.en.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vecs(lang, ft_vecs):\n",
    "    vecd = {w:ft_vecs.get_word_vector(w) for w in ft_vecs.get_words()}\n",
    "    pickle.dump(vecd, open('data/translate/wiki.en.pkl','wb'))\n",
    "    return vecd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecd = get_vecs('en', en_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecd = pickle.load(open('data/translate/wiki.en.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519370"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_words = en_vecs.get_words(include_freq=True)\n",
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}\n",
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])\n",
    "\n",
    "len(ft_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vector for a comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_en_vec = len(en_vecd[','])\n",
    "dim_en_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3874772e-05, 0.016063333)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vecs = np.stack(list(en_vecd.values()))\n",
    "en_vecs.mean(),en_vecs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 104)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nelen_90 = int(np.percentile([len(o) for o in ne_ids], 90))\n",
    "ollen_90 = int(np.percentile([len(o) for o in ol_ids], 97))\n",
    "nelen_90,ollen_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_ids_tr = np.array([o[:nelen_90] for o in ne_ids])\n",
    "ol_ids_tr = np.array([o[:ollen_90] for o in ol_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __getitem__(self, idx): return A(self.x[idx], self.y[idx])\n",
    "    def __len__(self): return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90569, 10093)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "trn_keep = np.random.rand(len(ne_ids_tr))>0.1\n",
    "ne_trn,ol_trn = ne_ids_tr[trn_keep],ol_ids_tr[trn_keep]\n",
    "ne_val,ol_val = ne_ids_tr[~trn_keep],ol_ids_tr[~trn_keep]\n",
    "len(ne_trn),len(ne_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = Seq2SeqDataset(ol_trn,ne_trn)\n",
    "val_ds = Seq2SeqDataset(ol_val,ne_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_samp = SortishSampler(ne_trn, key=lambda x: len(ne_trn[x]), bs=bs)\n",
    "val_samp = SortSampler(ne_val, key=lambda x: len(ne_val[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH= Path ('fastai/courses/andrea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_ds, bs, transpose=True, transpose_y=True, num_workers=1, \n",
    "                    pad_idx=1, pre_pad=False, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, int(bs*1.6), transpose=True, transpose_y=True, num_workers=1, \n",
    "                    pad_idx=1, pre_pad=False, sampler=val_samp)\n",
    "md = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(104, 52), (104, 52), (104, 52), (32, 43), (8, 8)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(trn_dl)\n",
    "its = [next(it) for i in range(5)]\n",
    "[(len(x),len(y)) for x,y in its]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(vecs, itos, em_sz):\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n",
    "    wgts = emb.weight.data\n",
    "    miss = []\n",
    "    for i,w in enumerate(itos):\n",
    "        try: wgts[i] = torch.from_numpy(vecs[w]*3)\n",
    "        except: miss.append(w)\n",
    "    print(len(miss),miss[5:10])\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh,nl = 276,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_loss(input, target):\n",
    "    sl,bs = target.size()\n",
    "    sl_in,bs_in,nc = input.size()\n",
    "    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n",
    "    input = input[:sl]\n",
    "    return F.cross_entropy(input.view(-1,nc), target.view(-1))#, ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqStepper(Stepper):\n",
    "    def step(self, xs, y, epoch):\n",
    "        self.m.pr_force = (10-epoch)*0.1 if epoch<10 else 0\n",
    "        xtra = []\n",
    "        output = self.m(*xs, y)\n",
    "        if isinstance(output,tuple): output,*xtra = output\n",
    "        self.opt.zero_grad()\n",
    "        loss = raw_loss = self.crit(output, y)\n",
    "        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n",
    "        loss.backward()\n",
    "        if self.clip:   # Gradient clipping\n",
    "            nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n",
    "        self.opt.step()\n",
    "        return raw_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_t(*sz): return torch.randn(sz)/math.sqrt(sz[0])\n",
    "def rand_p(*sz): return nn.Parameter(rand_t(*sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_All(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25, bidirectional=True)\n",
    "        self.out_enc = nn.Linear(nh*2, em_sz_dec, bias=False)\n",
    "        self.drop_enc = nn.Dropout(0.25)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "\n",
    "        self.W1 = rand_p(nh*2, em_sz_dec)\n",
    "        self.l2 = nn.Linear(em_sz_dec, em_sz_dec)\n",
    "        self.l3 = nn.Linear(em_sz_dec+nh*2, em_sz_dec)\n",
    "        self.V = rand_p(em_sz_dec)\n",
    "\n",
    "    def forward(self, inp, y=None):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = h.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(2,bs,-1)\n",
    "        h = self.out_enc(self.drop_enc(h))\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res,attns = [],[]\n",
    "        w1e = enc_out @ self.W1\n",
    "        for i in range(self.out_sl):\n",
    "            w2h = self.l2(h[-1])\n",
    "            u = F.tanh(w1e + w2h)\n",
    "            a = F.softmax(u @ self.V, 0)\n",
    "            attns.append(a)\n",
    "            Xa = (a.unsqueeze(2) * enc_out).sum(0)\n",
    "            emb = self.emb_dec(dec_inp)\n",
    "            wgt_enc = self.l3(torch.cat([emb, Xa], 1))\n",
    "            \n",
    "            outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "        return torch.stack(res)\n",
    "\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl*2, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 ['2015', '9', 'midlung', 'r.', '20']\n",
      "36 ['2015', '9', 'midlung', 'r.', '20']\n"
     ]
    }
   ],
   "source": [
    "rnn = Seq2SeqRNN_All(en_vecd, ol_itos, dim_en_vec, en_vecd, ne_itos, dim_en_vec, nh, nelen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d84b715d4e24d6f83a0ad7bdadf5c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      0.214791   2.272808  \n",
      "    1      0.13531    1.069795                                 \n",
      "    2      0.075025   0.385344                                  \n",
      "    3      0.045793   0.097252                                  \n",
      "    4      0.038328   0.037351                                  \n",
      "    5      0.031759   0.017906                                  \n",
      "    6      0.026524   0.020483                                  \n",
      "    7      0.050914   0.010068                                  \n",
      "    8      0.086006   0.01084                                   \n",
      "    9      0.046962   0.007734                                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.00773])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 1, cycle_len=10, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some printed output, top result is the original report, next is the goal translation from the word replacement, last is the ai training generated translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tubes and lines : none . lungs / pleura : lung volumes are slightly low , which results in mild crowding of the perihilar vasculature . there is no new focal airspace consolidation . no pleural effusion or pneumothorax is identified . a calcified nodule in the periphery of the right lung is unchanged . mediastinum : the cardiomediastinal silhouette is at the upper limits of normal for size . other : no displaced rib fracture is identified , however in the setting of chest pain if there is high clinical suspicion for rib injury , a dedicated rib series is recommended . _eos_\n",
      "tubes and lines : none . lungs / pleura : lung volumes are slightly low , which results in mild crowding of the perihilar vessels . there is no new focal airspace consolidation . no fluid around the lungs or partial collapse of lung is identified . a calcified nodule in the\n",
      "tubes and lines : none . lungs / pleura : lung volumes are slightly low , which results in mild crowding of the perihilar vessels . there is no new focal airspace consolidation . no fluid around the lungs or partial collapse of lung is identified . a calcified nodule in the\n",
      "\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "\n",
      "tubes and lines : none . lungs / pleura : lung volumes are slightly low , which results in mild crowding of the perihilar vasculature . there is no new focal airspace consolidation . no pleural effusion or pneumothorax is identified . a calcified nodule in the periphery of the right lung is unchanged . mediastinum : the cardiomediastinal silhouette is at the upper limits of normal for size . other : no displaced rib fracture is identified , however in the setting of chest pain if there is high clinical suspicion for rib injury , a dedicated rib series is recommended . _eos_\n",
      "tubes and lines : none . lungs / pleura : lung volumes are slightly low , which results in mild crowding of the perihilar vessels . there is no new focal airspace consolidation . no fluid around the lungs or partial collapse of lung is identified . a calcified nodule in the\n",
      "tubes and lines : none . lungs / pleura : lung volumes are slightly low , which results in mild crowding of the perihilar vessels . there is no new focal airspace consolidation . no fluid around the lungs or partial collapse of lung is identified . a calcified nodule in the\n",
      "\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "\n",
      "tubes and lines : none . lungs / pleura : lung volumes are slightly low , which results in mild crowding of the perihilar vasculature . there is no new focal airspace consolidation . no pleural effusion or pneumothorax is identified . a calcified nodule in the periphery of the right lung is unchanged . mediastinum : the cardiomediastinal silhouette is at the upper limits of normal for size . other : no displaced rib fracture is identified , however in the setting of chest pain if there is high clinical suspicion for rib injury , a dedicated rib series is recommended . _eos_\n",
      "tubes and lines : none . lungs / pleura : lung volumes are slightly low , which results in mild crowding of the perihilar vessels . there is no new focal airspace consolidation . no fluid around the lungs or partial collapse of lung is identified . a calcified nodule in the\n",
      "tubes and lines : none . lungs / pleura : lung volumes are slightly low , which results in mild crowding of the perihilar vessels . there is no new focal airspace consolidation . no fluid around the lungs or partial collapse of lung is identified . a calcified nodule in the\n",
      "\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "new round opacity projecting over the right midlung . recommend ct of the chest for further evaluation . these findings were discussed with dr t_up amar r. t_up patel , md on 9 / 20 / 2015 6:22 pm by dr . minkowitz , and read back verification was obtained . _eos_\n",
      "\n",
      "no focal consolidation , pleural effusion , or pneumothorax is identified . bilateral apical thickening left greater than right . the cardiomediastinal silhouette is within normal limits . focal increased density projecting over the proximal right clavicle and fourth rib , which could be related to a bony abnormality . clinical correlation and comparison with prior imaging is recommended to determine the need for additional imaging . _eos_\n",
      "no focal consolidation , fluid around the lungs , or partial collapse of lung is identified . bilateral apical thickening left greater than right . the heart shadow is within normal limits . focal increased density projecting over the proximal right clavicle and fourth rib , which could be related to a\n",
      "no focal consolidation , fluid around the lungs , or partial collapse of lung is identified . bilateral apical thickening left greater than right . the heart shadow is within normal limits . focal increased density projecting over the proximal right clavicle and fourth rib , which could be related to a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(val_dl))\n",
    "probs = learn.model(V(x))\n",
    "preds = to_np(probs.max(2)[1])\n",
    "\n",
    "for i in range(120,128):\n",
    "    print(' '.join([ol_itos[o] for o in x[:,i] if o != 1]))\n",
    "    print(' '.join([ne_itos[o] for o in y[:,i] if o != 1]))\n",
    "    print(' '.join([ne_itos[o] for o in preds[:,i] if o!=1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For MT-ComparEval File Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making two text files from the original text and the generated ai translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(val_dl))\n",
    "probs = learn.model(V(x))\n",
    "preds = to_np(probs.max(2)[1])\n",
    "s=open('source2.txt', 'w')\n",
    "r=open('ref2.txt', 'w')\n",
    "for i in range(1,128):\n",
    "    s.write(' '.join([ol_itos[o] for o in x[:,i] if o != 1]))\n",
    "    s.write(\"\\n\")\n",
    "    r.write(' '.join([ne_itos[o] for o in preds[:,i] if o!=1]))\n",
    "    r.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
